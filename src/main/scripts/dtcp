#!/bin/bash
# DataTorrent Gateway management utility.
# For more info see docs on http://www.datatorrent.com/
#
# Authors: Sandeep Deshmukh (sandeep@datatorrent.com)
#          Devendra Vyavhare (devendra@datatorrent.com)
#   
# Copyright: (c) 2014 DataTorrent, Inc. All rights reserved.


# Disable filename expansion
set -f

#------------------------------------------------------------------------------
# Support functions
#------------------------------------------------------------------------------
log() { printf "%b\n" "$*"; }
info() { log "$*" ; }
warn() { log "WARNING: $*" ; }
debug() { (( ${verbose} )) && log "DEBUG: $*"; }
error() { log "\nERROR: $*\n" ; }

# Produces real directory based on input with all the links resolved
# Following http://stackoverflow.com/questions/59895/can-a-bash-script-tell-what-directory-its-stored-in
real_dir() {
  SOURCE="${1:-${BASH_SOURCE[0]}}"
  while [ -h "$SOURCE" ]; do # resolve $SOURCE until the file is no longer a symlink
    SOURCE_DIR="$( cd -P "$( dirname "$SOURCE" )" && pwd )"
    SOURCE="$(readlink "$SOURCE")"
    [[ $SOURCE != /* ]] && SOURCE="$SOURCE_DIR/$SOURCE" # if $SOURCE was a relative symlink, we need to resolve it relative to the path where the symlink file was located
  done
  SOURCE_DIR="$( cd -P "$( dirname "$SOURCE" )" && pwd )"
  echo "$SOURCE_DIR"
}

#------------------------------------------------------------------------------
# Default settings
#------------------------------------------------------------------------------
fg_mode=0
service_mode=0
script_dir=$(real_dir "${BASH_SOURCE[0]}")
script_name=$(basename "${BASH_SOURCE[0]}")

# Provide instructions for using the installation script
usage(){
cat << USAGE

DataTorrent Ingestion Application"

Usage:
$0 [-f filefilter] [--fastMerge] [--scanInterval scanInterval] [--originalAppId appId] [-w] [--no-recurse] [--name] [--compact] [--compactionBundleName] [--compactionSize] [--separator] [-e-aes] [-e-pki] [--key key] [-c-gzip] [-c-lzo] [--lzoClassName lzoClassName] -j IngestionAppPackge sourceURL(s) destinationURL

 -j : IngestionApp jar file
 -f : filter for reading files from input source (default=read all files)
 --name : name of the application (has to be unique amongst running apps)
 --no-recurse : disable recursive copy of input data
 --fastMerge : enable fast merge (only on HDFS where IngestionAPP is running)
 --oneTimeCopy : shutdown application once copy is over
 --scanInterval : directory scan interval in h|m|s format e.g. 10h or 10m or 10s
 --originalAppId : specify original application identifier for restart
 -w : overwrite if file exists in destination directory (default=off)
 --compact : set compaction 
 --compactionBundleName : compaction bundle name
 --compactionSize : compaction size in  MB|GB format e.g. 2048MB or 5GB
 --separator : separator string to be used for file boundaries
 -e-aes : Set encryption to AES
 -e-pki : Set encryption to PKI
 --key : key to be used for encryption
 -c-gzip : set compression to gzip
 -c-lzo : set compression to lzo
 --lzoClassName : Class name of LzoOutputStream, this should extend com.datatorrent.apps.ingestion.process.LzoOutputStream class

USAGE
}


process_options() {
    while true; do
        case "$1" in
            -j|--jarfile) DT_APP=${2}; shift; shift;;
            -f|--filter) filter=${2}; shift; shift;;
            --no-recurse) recursive=false; shift;;
            --fastMerge) fastMerge=true; shift;;
            --oneTimeCopy) oneTimeCopy=true; shift;;
            --name) name=${2}; shift; shift;;
            --scanInterval) scanInterval=${2}; shift; shift;;
            --originalAppId) originalAppId=${2}; shift; shift;;
            -w|--overwrite) overwrite=true; shift;;
            --compact) compact=true; shift;;
            --compactionBundleName) compactionBundleName=${2}; shift; shift;;
            --compactionSize) compactionSize=${2}; shift; shift;;
            --separator) separator=${2}; shift; shift;;
            -e-aes) aes=true; shift;;
            -e-pki) pki=true; shift;;
            --key) key=${2}; shift; shift;;
            -c-gzip) gzipCompress=true; shift;;
            -c-lzo) lzoCompress=true; shift;;
            --lzoClassName) lzoClassName=${2}; shift; shift;;
            *) open_string=$open_string" "${1}; if [ "${1}" == "" ];then break; fi; shift;;
        esac
    done
}


load_dt_env() {

    # Load DataTorrent environment settings
    # DT_HADOOP is searched or loaded from user settings via sfw-env.sh
    # DT_JAVA is searched or loaded from user settings via sfw-env.sh
    # DT_CLASSPATH is loaded via dt-env.sh
    for conf_dir in "${script_dir}/../conf" "$HOME/.dt"; do
        if [[ -f "${conf_dir}/dt-env.sh" ]]; then
            DT_CONF_DIR="${conf_dir}"
            DT_ENV_SH="${conf_dir}/dt-env.sh"
            . "${DT_ENV_SH}"
        fi
    done

    # Set defaults in case environment settings are missing
    DT_BASE_DIR=${DT_BASE_DIR:-"$HOME/.dt"}
    DT_RUN_DIR=${DT_RUN_DIR:-"$HOME/.dt/run"}
    DT_LOG_DIR=${DT_LOG_DIR:-"$HOME/.dt/logs"}
    DATATORRENT_HOME=${DATATORRENT_HOME:-"$DT_BASE_DIR/current"}
    [[ -d ${DATATORRENT_HOME}/ ]] || DATATORRENT_HOME="${script_dir}/.."

    # In development mode, if configuration files are not found, locate DT_HADOOP manually
    if [ -z "${DT_HADOOP}" ]; then
      HADOOP_SEARCH_PATH="${HADOOP_PREFIX}/bin:${HADOOP_HOME}/bin:${PATH}:/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:."
      DT_HADOOP=`PATH=${HADOOP_SEARCH_PATH} && command -v hadoop 2>/dev/null`
    fi

    # If gateway is not running as service, and local ~/.dt/dt-env.sh override is missing
    # change run and logs directories to be local
    if [ ${service_mode} -ne 1 ] &&  ! [ -f "$HOME/.dt/dt-env.sh" ]; then
        # debug "Setting logs and run directory base to $HOME/.dt due to missing $HOME/.dt/dt-env.sh"
        DT_LOG_DIR="${HOME}/.dt/logs"
        DT_RUN_DIR="${HOME}/.dt/run"
    fi


}

set_dtcp_env() {
    
    # Load DataTorrent environment files
    load_dt_env

    # Ensure DATATORRENT_HOME is set and exported - used by DTGateway
    export DATATORRENT_HOME

    # Define classpaths
    DT_DTCP_CLASSPATH="${DT_CLASSPATH}"

    # Add development support by loading maven classpath settings
    BUILD_DIR="$( cd ${script_dir}/../../../target 2>/dev/null && pwd -P)"
    MVN_GENERATED_PATH="$BUILD_DIR/mvn-generated-runtime-classpath-no-hadoop"
    if [ -f "$MVN_GENERATED_PATH" ]; then
        #info "Development launch mode.  Setting classpath from: $MVN_GENERATED_PATH"
        DT_DTCP_CORE_JAR=`ls $BUILD_DIR/DTApp-ingestion-app*.jar | grep -Ev "tests.jar|javadoc.jar|sources.jar" | tail -1`
        if [ -f "$DT_DTCP_CORE_JAR" ]; then
            DT_DTCP_CLASSPATH="$DT_DTCP_CLASSPATH:$DT_DTCP_CORE_JAR"
        else
            error "Cannot find $DT_DTCP_CORE_JAR";
            exit 1;
        fi
        if [ ! -x "$DT_HADOOP" ]; then
            error "Unable to continue due to missing hadoop executable.  Please ensure it is available in PATH.";
            exit 1;
        fi
        DT_DTCP_CLASSPATH="$DT_DTCP_CLASSPATH:`cat $MVN_GENERATED_PATH`"
    else
      DT_DTCP_CLASSPATH="$DATATORRENT_HOME/lib"'/*'":${DT_DTCP_CLASSPATH}"
    fi

    # Add missing $script_dir directories only if not already in $PATH
    echo "$PATH" | grep "$script_dir" > /dev/null
    if [[ $? -ne 0 ]]; then
        export PATH=$script_dir/../../../../stram/src/main/scripts:$script_dir:$PATH
    fi

}


set_relaunch_parameters() {
  inputurl="dummyUrl"
  outputurl="dummyurl"
}

check_parameters(){

# Validate input JAR
if [ "$DT_APP" == "" ]
then
    echo "JAR file name missing"
    usage;
    echo ;
    exit 1 ;
fi

# parse open_string
words=( $open_string )
len="${#words[@]}"
if [ $len -gt 1 ];
then
  inputurl=${words[0]};
  outputurl=${words[$len-1]}
fi

end=$(($len-2))
for x in `seq 1 $end`;
do
    inputurl=$inputurl","${words[$x]};
done;

# Validate output URL
if [ "$outputurl" == "" ]
then
    echo "Mandatory output URL is missing. Exiting".
    usage ;
    exit 1 ;
fi

# Validate input URL
if [ "$inputurl" == "" ]
then
    echo "Mandatory input URL is missing."
    usage;
    echo ;
    exit 1 ;
fi

if [[ ! -z "$aes" && ! -z "$pki" ]]
then
  echo "Select only one encryption method at a time."
  exit 1;
fi

if [[ ! -z "$gzipCompress" && ! -z "$lzoCompress" ]]
then
  echo "Select only one compression method at a time."
  exit 1;
fi

input_protocol=`echo $inputurl | awk -F":" '{print $1}' `
output_protocol=`echo $outputurl | awk -F":" '{print $1}' `

if [ $input_protocol == "kafka" ]
then
  topic=`echo $inputurl | awk -F/ '{print $3}'`
  zookeeperlist=`echo $inputurl | awk -F/ '{print $4}'`
  if [ -z "$topic" -o -z "$zookeeperlist" ]
  then
    echo "Please provide valid input url for kafka. Valid url format: kafka://<topic>/<zookeeper-list> "
    exit 1
  fi
fi


if [ $input_protocol == "jms" ]
then
  brokerURL=`echo $inputurl | awk -F/ '{print $3}'`
  subject=`echo $inputurl | awk -F/ '{print $4}'`
  if [ -z "$subject" -o -z "$brokerURL" ]
  then
    echo "Please provide valid input url for jms. Valid url format: jms://host:port/topic"
    exit 1
  fi
fi

if [ ! -z "$originalAppId" ]
then
  launchArgs="$launchArgs -originalAppId $originalAppId";
fi
}

parse_scantime() {
  unit=${scanInterval: -1}
  case "$unit" in
    "h") sec=`expr 60 \* 60`;
    ;;
    "m") sec=60;
    ;;
    "s") sec=1;
    ;;
    *)
      echo "Enter valid unit for scanInterval it should be in either h, m or s e.g. 10h or 10m or 10s" >&2
      exit 1;
    ;;
  esac
  time=${scanInterval:0:${#scanInterval}-1}
  re='^[0-9]+$'
  if ! [[ $time =~ $re ]]
  then
   echo "Enter valid number for scanInterval" >&2
   exit 1;
  fi
  totalMS=`expr $time \* $sec \* 1000`
  echo $totalMS
}

parse_compactionSize() {
  if [ -z "$compactionSize" ]
  then
    echo "compactionSize is mandatory if compaction is enabled. Please enter valid compactionSize." >&2
    exit 1;
  fi
  unit=${compactionSize: -2}
  case "$unit" in
    "MB") bytes=`expr 1024 \* 1024`;
    ;;
    "GB") bytes=`expr 1024 \* 1024 \* 1024`;
    ;;
    *)
      echo "Enter valid unit for compactionSize it should be in either MB or GB e.g. 2048MB or 5GB" >&2
      exit 1;
    ;;
  esac
  size=${compactionSize:0:${#compactionSize}-2}
  re='^[0-9]+$'
  if ! [[ $size =~ $re ]]
  then
   echo "Enter valid number for compactionSize" >&2
   exit 1;
  fi
  totalSize=`expr $size \* $bytes`
  echo $totalSize
}

create_xml(){

CONF_FILE=/tmp/dtcp.$MYPID;
touch $CONF_FILE;
if [ $? -ne 0 ]
then
  echo "Unable to create tmp file $CONF_FILE . Exiting."
  exit 1;
fi


cat  > $CONF_FILE << EOF
<configuration>
  <property>
    <name>dt.operator.FileSplitter.prop.scanner.files</name>
    <value>$inputurl</value>
  </property>

  <property>
    <name>dt.operator.FileMerger.prop.filePath</name>
    <value>$outputurl</value>
  </property>

EOF

if [ ! -z "$filter" ]
then
cat >> $CONF_FILE << FILTER
  <property>
    <name>dt.operator.FileSplitter.prop.scanner.filePatternRegularExp</name>
    <value>$filter</value>
  </property>
FILTER
fi

if [ ! -z "$recursive" ]
then
cat >> $CONF_FILE << RECURSIVE
  <property>
    <name>dt.operator.FileSplitter.prop.scanner.recursive</name>
    <value>$recursive</value>
  </property>
RECURSIVE
fi


if [ ! -z "$overwrite" ]
then
cat >> $CONF_FILE << OVERWRITE
  <property>
    <name>dt.operator.FileMerger.prop.overwriteOutputFile</name>
    <value>$overwrite</value>
  </property>
OVERWRITE
fi

if [ ! -z "$aes" ]
then
cat >> $CONF_FILE << AESENCRYPT
  <property>
    <name>dt.application.Ingestion.encrypt.aes</name>
    <value>$aes</value>
  </property>
AESENCRYPT
encrypt=true
fi

if [ ! -z "$pki" ]
then
cat >> $CONF_FILE << PKIENCRYPT
  <property>
    <name>dt.application.Ingestion.encrypt.pki</name>
    <value>$pki</value>
  </property>
PKIENCRYPT
encrypt=true
fi

if [ "$encrypt" = true ]
then
  if [ -z "$key" ]
  then
    echo "Please provide the encryption key for selected encryption type."
    exit 1;
  fi
cat >> $CONF_FILE << PASSKEY
  <property>
    <name>dt.application.Ingestion.encrypt.key</name>
    <value>$key</value>
  </property>
PASSKEY
fi

if [ ! -z "$gzipCompress" -o ! -z "$lzoCompress" ]
then
cat >> $CONF_FILE << COMPRESS
  <property>
    <name>dt.application.Ingestion.compress</name>
    <value>true</value>
  </property>
COMPRESS
fi

if [ ! -z "$gzipCompress" ]
then
cat >> $CONF_FILE << GZCOMPRESS
  <property>
    <name>dt.application.Ingestion.compress.type</name>
    <value>gzip</value>
  </property>
GZCOMPRESS
fi

if [ ! -z "$lzoCompress" ]
then
  if [ -z "$lzoClassName" ]
  then
    echo "Please provide the class name for lzoOuputStream implementation"
    exit 1;
  fi
cat >> $CONF_FILE << LZOCOMPRESS
  <property>
    <name>dt.application.Ingestion.compress.type</name>
    <value>lzo</value>
  </property>
  <property>
    <name>dt.application.Ingestion.compress.lzo.className</name>
    <value>$lzoClassName</value>
  </property>
LZOCOMPRESS
fi

if [ ! -z "$compact" ]
then
compactSize=$( parse_compactionSize )
  if [ -z "$compactionBundleName" ]
  then
    echo "compactionBundleName is mandatory if compaction is enabled. Please enter valid compactionBundleName." >&2
    exit 1;
  fi
cat >> $CONF_FILE << COMPACT
  <property>
    <name>dt.application.Ingestion.compact</name>
    <value>$compact</value>
  </property>
  <property>
    <name>dt.application.Ingestion.operator.PartitionMetaDataEmitter.partitionSizeInBytes</name>
    <value>$compactSize</value>
  </property>
  <property>
    <name>dt.application.Ingestion.compact.separator</name>
    <value>$separator</value>
  </property>
  <property>
    <name>dt.application.Ingestion.operator.PartitionMetaDataEmitter.compactionBundleName</name>
    <value>$compactionBundleName</value>
  </property>
  <property>
    <name>dt.application.Ingestion.operator.MetaFileWriter.compactionBundleName</name>
    <value>$compactionBundleName</value>
  </property>
  <property>
    <name>dt.application.Ingestion.operator.PartitionWriter.prop.filePath</name>
    <value>$outputurl</value>
  </property>
  <property>
    <name>dt.application.Ingestion.operator.MetaFileWriter.prop.filePath</name>
    <value>$outputurl</value>
  </property>
COMPACT
fi

case $input_protocol in 
    "kafka")
    cat >> $CONF_FILE << KAFKA
  <property>
    <name>dt.application.Ingestion.operator.MessageReader.prop.topic</name>
    <value>$topic</value>
  </property>
  <property>
    <name>dt.application.Ingestion.operator.MessageReader.prop.zookeeper</name>
    <value>$zookeeperlist</value>
  </property>
  <property>
    <name>dt.application.Ingestion.operator.FileWriter.prop.filePath</name>
    <value>$outputurl</value>
  </property>
KAFKA
;;
    "jms")
    cat >> $CONF_FILE << JMS
  <property>
    <name>dt.application.Ingestion.operator.MessageReader.prop.connectionFactoryProperties.brokerURL</name>
    <value>tcp://$brokerURL</value>
  </property>
  <property>
    <name>dt.application.Ingestion.operator.MessageReader.prop.ackMode</name>
    <value>AUTO_ACKNOWLEDGE</value>
  </property>
  <property>
    <name>dt.application.Ingestion.operator.MessageReader.prop.subject</name>
    <value>$subject</value>
  </property>
  <property>
    <name>dt.application.Ingestion.operator.FileWriter.prop.filePath</name>
    <value>$outputurl</value>
  </property>
JMS
;;

    "hdfs")
cat >>  $CONF_FILE << HDFS
  <property>
    <name>dt.operator.FileSplitter.prop.scanner.ignoreFilePatternRegularExp</name>
    <value>.*\._COPYING_</value>
  </property>
HDFS
;;
    
esac # end of case statement for input_protocol


if [ ! -z "$input_protocol" ]
then
cat >>  $CONF_FILE << INPUTPROTOCOL
  <property>
    <name>dt.operator.BlockReader.prop.scheme</name>
    <value>$input_protocol</value>
  </property>
INPUTPROTOCOL
fi

cat >> $CONF_FILE << OUTPUTPROTOCOL
  <property>
    <name>dt.output.protocol</name>
    <value>$output_protocol</value>
  </property>
OUTPUTPROTOCOL

if [ ! -z "$fastMerge" ]
then
cat >> $CONF_FILE << FASTMERGE
  <property>
    <name>dt.output.enableFastMerge</name>
    <value>$fastMerge</value>
  </property>
FASTMERGE
fi

if [ ! -z "$oneTimeCopy" ]
then
cat >> $CONF_FILE << ONETIMECOPY
  <property>
    <name>dt.input.oneTimeCopy</name>
    <value>$oneTimeCopy</value>
  </property>
ONETIMECOPY
fi


if [ ! -z "$scanInterval" ]
then
interval=$( parse_scantime )
if [[ $? != 0 ]]
then
  exit 1;
fi
cat >> $CONF_FILE << EOF
  <property>
    <name>dt.operator.FileSplitter.prop.scanner.scanIntervalMillis</name>
    <value>$interval</value>
  </property>
EOF
fi

if [ ! -z "$name" ]
then
cat >> $CONF_FILE << NAME
  <property>
    <name>dt.application.Ingestion.attr.APPLICATION_NAME</name>
    <value>Ingestion-$name</value>
  </property>
NAME
fi



echo "</configuration>" >> $CONF_FILE 
}


launch_app() {
  # Search for Ingestion application name. This needs to move to setupDtcpEnv() when that is integrated with the script.
  DT_INGESTION_APP=${DT_INGESTION_APP:=Ingestion}
  echo ;
  echo ;
  echo "Launching app..."
  echo launch -conf $CONF_FILE $launchArgs $DT_APP -exactMatch $DT_INGESTION_APP ;
  if [ ! -z "$DTCP_TEST_MODE" ]
  then
    # Test  mode will only create the conf file and will skip launching the app.
    # Used for testing dtcp script (internal, not exposed to the users)
    exit 0;
  else
    echo launch -conf $CONF_FILE $launchArgs $DT_APP -exactMatch $DT_INGESTION_APP | dtcli
  fi
  if [ $? != 0 ]
  then
    echo "Error running Ingestion App. Exiting...";
  fi
}



#------------------------------------------------------------------------------
# Process user input and execute requested action
#------------------------------------------------------------------------------
# Confirm at at least one argument was provided
if [ -z $1 ]; then
  usage;
  exit 1;
fi

script_opts="$@"

# Process remaining action options
process_options ${script_opts}
if [ ! -z "$originalAppId" ]
then
  set_relaunch_parameters
fi
check_parameters

script_dir=$(real_dir "${DT_APP}")
script_name=$(basename "${DT_APP}")

MYPID=$BASHPID;

create_xml

launch_app

# Revert filename expansion changes.
set +f

